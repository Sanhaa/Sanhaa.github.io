---
layout: post
title: NLP | Bag of Words
description: >
  
# sitemap: false
hide_last_modified: true
---

- Table of Contents
{:toc .large-only}


## Bag of words

문서

- 자주 발생하는 단어가 문서의 특징을 나타낸다는 것을 가정
- 문서 벡터의 차원은 데이터 내 발생하는 모든 단어의 개수와 동일
- 합성어를 독립적인 단어로 개별처리 (ex. log off)


## N-grams
> 연속된 N개의 단어를 기준으로 텍스트 분석을 수행


## TF-IDF
> 상대적으로 자주 발생하는 단어가 더 중요하다는 점을 반영

- 단순한 빈도수가 아니라 상대적 빈도수가 중요하다
- (특정 단어 빈도수) / (전체 단어 빈도수) * log (데이터 내 총 문서의 개수/데이터 내 특정 단어가 들어간 문서의 개수)
- 모든 문서에서 전반적으로 나타나는 단어의 점수를 줄여줌 (ex. 그러나, 그리고 등)

~~~python
from sklearn.feature_extraction.text import TfidfVectorizer

# TfidfVectorizer을 불러옵니다. (stop_words 는 영어로 설정)
vectorizer = TfidfVectorizer(stop_words = 'english')

X = vectorizer.fit_transform(df_clean['Review Text'].str.lower())
~~~