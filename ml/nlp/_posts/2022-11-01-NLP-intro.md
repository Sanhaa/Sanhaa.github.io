---
layout: post
title: NLP | 자연어 처리 기본 흐름
description: >
  NLP 기본 흐름
# sitemap: false
hide_last_modified: true
---

- Table of Contents
{:toc .large-only}



## 1. 텍스트 전처리
> 줄글 텍스트에서 단어 뽑아내기

1) 특수 문자 제거
    `hi`와  `hi!`를 같은 단어로 처리
2) stopword 제거
    `is`, `the`와 같이 의미보다는 문법적 기능을 가진 단어 제거
    *노이즈 제거와 같음*
3) stemming
    `love`, `loved`, `loves`가 모두 같은 `love`로 처리 되도록 단어의 공통된 형태로 변환
    *normalize(정규화)와 같음*


## 단어 임베딩

> 각 단어를 연속형 벡터로 표현하는 방법

컴퓨터는 텍스트를 어떻게 인식해야할까?
수치형 데이터로 어떻게 mapping 할까?

~~~
나무 --> [0.23, 0.44, 0.21 ...] 
꽃 --> [0.29, 0.40, 0.8 ...]
이다 --> [0.913, 0.879, ...]
~~~

= 문맥을 이용


임베딩 벡터 간 합과 차로 단어의 의미적 특징을 활용할 수 있음

### word2vec
> 단어 임베딩 벡터 학습 알고리즘

주어진 문맥에서 발생하는 단어를 예측하는 문제를 통해 단어 임베딩 벡터를 학습

특정 단어들이 입력으로 주어졌을 때 계산되는 은닉층의 값이 `임베딩 벡터`가 된다

- word2vec이 구현된 여러 라이브러리가 존재
ex. gensim 파이썬 라이브러리

~~~python
from gensim.models import Word2Vec

# window: 문맥의 범위, 앞뒤 단어 개수
# vector_size: hidden node 개수, 벡터의 차원수
wv_model =Word2Vec( window=2, vector_size=300)

# input으로 쓰이는 단어들을 정수형 id로 변환하여 더욱 빠르게 학습
wv_model.build_vocab(input_data) 

wv_model.train(input_data, total_examples=wv_model.corpus_count, epochs=10)

# "sad"와 유사한 단어 
similar_sad = wv_model.wv.most_similar("sad")
'''
[('unhappy', 0.947895884513855), ('scared', 0.9425392746925354), ('hopeless', 0.9322786927223206), ('lonely', 0.9289621710777283), ('needy', 0.9200672507286072), ('paranoid', 0.9122549295425415), ('bitchy', 0.9073493480682373), ('angry', 0.9071397185325623), ('depressed', 0.9064767956733704), ('insecure', 0.9049254059791565)]
'''

# 두 단어의 임베딩 벡터 간 유사도
# 코사인 유사도
similar_sad_lonely = wv_model.wv.similarity("sad", "lonely")
'''
0.9289622
'''
~~~